# Unilever Data Warehouse Pipeline

A comprehensive ETL pipeline for the Unilever data warehouse project. This is a production-ready data engineering portfolio project demonstrating end-to-end data warehouse implementation.

## Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        UNILEVER DATA WAREHOUSE PIPELINE                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                           DATA FLOW DIAGRAM
                           =================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   SOURCE     â”‚     â”‚   STAGING    â”‚     â”‚   ETL        â”‚     â”‚   WAREHOUSE  â”‚
â”‚   SYSTEMS    â”‚â”€â”€â”€â”€â–¶â”‚   AREA       â”‚â”€â”€â”€â”€â–¶â”‚   PIPELINE   â”‚â”€â”€â”€â”€â–¶â”‚   (PostgreSQL)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                    â”‚                    â”‚                    â”‚
       â–¼                    â–¼                    â–¼                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CSV Files    â”‚     â”‚ CSV Files    â”‚     â”‚ Python       â”‚     â”‚ Star Schema  â”‚
â”‚ JSON Files   â”‚     â”‚ (Products,   â”‚     â”‚ Scripts      â”‚     â”‚ Fact Table   â”‚
â”‚ Excel Files  â”‚     â”‚  Customers,  â”‚     â”‚ Airflow DAG â”‚     â”‚ Dimension    â”‚
â”‚ Database     â”‚     â”‚  Sales)      â”‚     â”‚ Shell Scriptsâ”‚     â”‚ Tables       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                                 â”‚
                                                                 â–¼
                                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                              â”‚   CONTROL & METADATA         â”‚
                                              â”‚  - load_batch (folders)      â”‚
                                              â”‚  - etl_log (runs)            â”‚
                                              â”‚  - data_quality_log          â”‚
                                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


                           STAR SCHEMA DESIGN
                           =================

                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                           â”‚  dim_date   â”‚
                           â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
                           â”‚ date_key PK â”‚
                           â”‚ sale_date   â”‚
                           â”‚ year        â”‚
                           â”‚ month       â”‚
                           â”‚ day         â”‚
                           â”‚ quarter     â”‚
                           â”‚ month_name  â”‚
                           â”‚ day_of_week â”‚
                           â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                                  â”‚ date_key
                                  â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                         â”‚                         â”‚
        â–¼                         â–¼                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ dim_product   â”‚          â”‚  fact_sales   â”‚          â”‚ dim_customer  â”‚
â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚          â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚          â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚ product_key PKâ—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚ product_key FK â”‚          â”‚ customer_key PKâ”‚
â”‚ product_id    â”‚          â”‚ customer_key FKâ”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚ customer_id   â”‚
â”‚ product_name  â”‚          â”‚ date_key FK    â”‚          â”‚ customer_name â”‚
â”‚ category      â”‚          â”‚ quantity       â”‚          â”‚ email         â”‚
â”‚ brand         â”‚          â”‚ revenue        â”‚          â”‚ city          â”‚
â”‚               â”‚          â”‚ sale_id (UNIQUE)         â”‚ province      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


                           ETL PIPELINE FLOW
                           ================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           ETL PROCESS FLOW                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. EXTRACT
   â”œâ”€â”€ Detect latest raw data folder (raw_data/YYYY-MM-DD/)
   â”œâ”€â”€ Copy files to staging area
   â”œâ”€â”€ Load CSV files into pandas DataFrames
   â””â”€â”€ Validate data formats

2. TRANSFORM
   â”œâ”€â”€ Data Quality Checks
   â”‚   â”œâ”€â”€ Check for null values
   â”‚   â”œâ”€â”€ Check for duplicates
   â”‚   â”œâ”€â”€ Check for outliers
   â”‚   â””â”€â”€ Check for negative values
   â”‚
   â”œâ”€â”€ Dimension Processing
   â”‚   â”œâ”€â”€ Deduplicate products
   â”‚   â”œâ”€â”€ Deduplicate customers
   â”‚   â”œâ”€â”€ Deduplicate dates
   â”‚   â””â”€â”€ SCD Type 2 (historical tracking)
   â”‚
   â””â”€â”€ Fact Processing
       â”œâ”€â”€ Create staging_sales table
       â”œâ”€â”€ Join with dimensions (surrogate keys)
       â””â”€â”€ Insert with ON CONFLICT handling

3. LOAD
   â”œâ”€â”€ Insert new dimension records
   â”œâ”€â”€ Insert fact records (idempotent)
   â”œâ”€â”€ Update metadata tables
   â”‚   â”œâ”€â”€ load_batch (folder tracking)
   â”‚   â”œâ”€â”€ etl_log (run tracking)
   â”‚   â””â”€â”€ data_quality_log (quality issues)
   â”‚
   â””â”€â”€ Archive processed raw data


                           TECHNICAL STACK
                           ==============

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           TECHNOLOGY STACK                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   DATA          â”‚    â”‚   ORCHESTRATION â”‚    â”‚   MONITORING   â”‚
â”‚   PROCESSING    â”‚    â”‚                 â”‚    â”‚                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Python 3.8+    â”‚    â”‚ Apache Airflow  â”‚    â”‚ Prometheus      â”‚
â”‚ Pandas          â”‚    â”‚ Cron Jobs       â”‚    â”‚ Grafana        â”‚
â”‚ SQLAlchemy      â”‚    â”‚ Shell Scripts   â”‚    â”‚ Custom Scripts  â”‚
â”‚ PostgreSQL      â”‚    â”‚                 â”‚    â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

         â”‚                      â”‚                      â”‚
         â–¼                      â–¼                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   DATA          â”‚    â”‚   DEPLOYMENT   â”‚    â”‚   DOCUMENTATIONâ”‚
â”‚   GENERATION    â”‚    â”‚                 â”‚    â”‚                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Faker Library   â”‚    â”‚ Docker          â”‚    â”‚ README          â”‚
â”‚ CSV/JSON/Excel  â”‚    â”‚ Docker Compose â”‚    â”‚ Architecture    â”‚
â”‚ Data Quality    â”‚    â”‚ GitHub          â”‚    â”‚ Diagrams        â”‚
â”‚ Issues          â”‚    â”‚                 â”‚    â”‚ Runbooks        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


                           PROJECT STRUCTURE
                           ================

unilever_pipeline/
â”‚
â”œâ”€â”€ ğŸ“ DATABASE SCHEMA
â”‚   â”œâ”€â”€ setup_warehouse.sql       # Star schema definition
â”‚   â””â”€â”€ setup_partitions.sql     # Table partitioning
â”‚
â”œâ”€â”€ ğŸ“ DATA GENERATION
â”‚   â””â”€â”€ generate_data.py          # Sample data with quality issues
â”‚
â”œâ”€â”€ ğŸ“ ETL PIPELINE
â”‚   â”œâ”€â”€ etl_load_staging.py       # Main ETL script (SCD Type 2)
â”‚   â””â”€â”€ run_pipeline.py           # Legacy pipeline runner
â”‚
â”œâ”€â”€ ğŸ“ ORCHESTRATION
â”‚   â”œâ”€â”€ ingest_data.sh            # Shell ingestion script
â”‚   â”œâ”€â”€ etl_dag.py               # Airflow DAG
â”‚   â””â”€â”€ cron_setup.sh            # Cron job configuration
â”‚
â”œâ”€â”€ ğŸ“ MONITORING
â”‚   â”œâ”€â”€ monitor_etl.py            # Pipeline monitoring
â”‚   â”œâ”€â”€ db_optimize.py           # Database optimization
â”‚   â””â”€â”€ alerts/                   # Alert configurations
â”‚
â”œâ”€â”€ ğŸ“ INFRASTRUCTURE
â”‚   â”œâ”€â”€ docker-compose.yml        # Docker Compose stack
â”‚   â”œâ”€â”€ .env.example             # Environment variables
â”‚   â””â”€â”€ requirements.txt          # Python dependencies
â”‚
â”œâ”€â”€ ğŸ“ DOCUMENTATION
â”‚   â”œâ”€â”€ README.md                 # This file
â”‚   â”œâ”€â”€ ARCHITECTURE.md          # Architecture details
â”‚   â””â”€â”€ OPERATIONS.md            # Operations runbook
â”‚
â””â”€â”€ ğŸ“ DATA
    â”œâ”€â”€ raw_data/                 # Raw data (daily folders)
    â”œâ”€â”€ staging/                  # Staging area
    â””â”€â”€ archive/                  # Archived data


                           QUICK START
                           ==========

Prerequisites:
- Python 3.8+
- PostgreSQL 14+
- Docker & Docker Compose (optional)

1. Clone the repository:
   git clone https://github.com/yourusername/unilever_pipeline.git
   cd unilever_pipeline

2. Set up environment:
   cp .env.example .env
   # Edit .env with your settings

3. Create virtual environment:
   python -m venv venv
   source venv/bin/activate  # Linux/Mac
   venv\Scripts\activate     # Windows

4. Install dependencies:
   pip install -r requirements.txt

5. Set up database:
   psql -U postgres -p 5433 -d unilever_warehouse -f setup_warehouse.sql

6. Generate sample data:
   python generate_data.py

7. Run ETL pipeline:
   python etl_load_staging.py

8. Or use Docker:
   docker-compose up -d


                           USAGE EXAMPLES
                           =============

# Generate data with data quality issues
python generate_data.py

# Run ETL pipeline
python etl_load_staging.py

# Monitor pipeline
python monitor_etl.py

# Optimize database
python db_optimize.py

# Run shell ingestion with monitoring
./ingest_data.sh --run

# Setup daily cron job
./ingest_data.sh --setup-cron


                           FEATURES
                           ========

âœ… Star Schema Design
  - Fact table: fact_sales
  - Dimensions: dim_product, dim_customer, dim_date
  - Surrogate keys and foreign keys

âœ… Data Quality
  - Intentional data quality issues for testing
  - Null value detection
  - Duplicate detection
  - Outlier detection
  - Data quality logging

âœ… ETL Pipeline
  - SCD Type 2 support for dimensions
  - Idempotent operations
  - Comprehensive logging
  - Error handling
  - Batch tracking

âœ… Orchestration
  - Apache Airflow DAG
  - Shell scripting with cron
  - Automated file monitoring
  - Email/Slack notifications

âœ… Monitoring & Logging
  - ETL run tracking
  - Data quality metrics
  - Performance monitoring
  - Alerting

âœ… Database Administration
  - Table partitioning by date
  - Index optimization
  - Vacuum and analyze
  - Backup procedures

âœ… Deployment
  - Docker Compose
  - Environment configuration
  - GitHub ready


                           MONITORING DASHBOARDS
                           =====================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         ETL MONITORING DASHBOARD                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PIPELINE STATUS â”‚  RECORDS LOADED  â”‚  DATA QUALITY    â”‚  PERFORMANCE     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ âœ… Success: 45   â”‚ Products: 12,500 â”‚ Issues: 234      â”‚ Avg: 45s         â”‚
â”‚ âŒ Failed: 3    â”‚ Customers: 45,000 â”‚ Nulls: 120       â”‚ Min: 23s         â”‚
â”‚ â³ Running: 1    â”‚ Dates: 730        â”‚ Duplicates: 114  â”‚ Max: 120s        â”‚
â”‚                  â”‚ Sales: 650,000   â”‚ Outliers: 50     â”‚                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


                           DATABASE SCHEMA
                           ==============

Dimension Tables:
- dim_product: Product information (surrogate key, natural key, attributes)
- dim_customer: Customer details (surrogate key, natural key, attributes)
- dim_date: Calendar hierarchy (date key, date, year, month, quarter, etc.)

Fact Tables:
- fact_sales: Sales transactions (sale_id, product_key, customer_key, date_key, quantity, revenue)

Control Tables:
- load_batch: Tracks processed folders (folder_name, status)
- etl_log: Pipeline execution logs (start_time, end_time, status, counts)
- data_quality_log: Quality issues (table_name, check_type, issue_count)


                           TROUBLESHOOTING
                           ==============

Common Issues:

1. Duplicate Key Error
   # Check existing runs
   SELECT * FROM etl_log ORDER BY start_time DESC LIMIT 5;
   
   # Check fact table
   SELECT COUNT(*) FROM fact_sales;

2. Missing Tables
   # Recreate schema
   psql -f setup_warehouse.sql

3. Data Quality Issues
   # Run monitoring
   python monitor_etl.py

4. Database Connection
   # Test connection
   psql -U postgres -p 5433 -d unilever_warehouse


                           CONTACT & LICENSE
                           =================

Author: [Your Name]
Email: contact@unilever-pipeline.com
License: MIT

For questions or contributions, please open an issue on GitHub.
